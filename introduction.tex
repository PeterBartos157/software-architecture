
Reproducibility, as defined by ACM \cite{ACMreproducibility}, 
is the ability to obtain precise measurements by different 
teams under the same conditions. In AI advancements, reproducibility 
is crucial, with Peng \cite{peng2011reproducible} considering it 
the ultimate measure of scientific validity. However, Pawlik et 
al. \cite{pawlik2019link} found that only 7.64\% of analyzed papers 
were reproducible. Improved data version control can enhance 
reproducibility and enable more complex studies, increasing understanding 
of neural networks. Pawlik et al. \cite{pawlik2019link} suggest that 
datasets should include not only input data but also raw data and 
preparation instructions to improve context and reproducibility. 
\\\\
To understand how data versioning integrates into machine learning 
architecture, it's essential to examine key components and workflows. 
Machine learning pipelines rely on iterative experimentation, where 
data is critical at every stage, from preprocessing to model training, 
evaluation, and deployment. Maintaining consistent, traceable versions 
of datasets and models is vital for ensuring reproducibility. 
\cite{wandb}